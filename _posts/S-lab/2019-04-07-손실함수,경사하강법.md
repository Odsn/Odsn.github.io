---
layout: post
title: 손실함수와 경사하강법
published: True
category: 실전프로젝트
permalink: /S-lab/:year/:month/:day/:title/
comments: true
---

## 손실함수, 경사하강법



### 손실 함수

 \- 손실 함수(cost function) : 손실 함수란 신경망이 학습할 수 있도록 해주는 지표이다. 머신러닝 모델의 출력값과 사용자가 원하는 출력값의 차이, 즉 오차를 말한다. 이 손실 함수 값이 최소화되도록 하는 가중치와 편향을 찾는 것이 바로 학습이다. 일반적인 손실 함수로 평균 제곱 오차나 교차 엔트로피 오차를 사용한다.





- 평균 제곱 오차

![img](https://t1.daumcdn.net/cfile/tistory/999E973A5A9273A405)



 \- 평균 제곱 오차(Mean Squared Error : MSE) : 계산이 간편하여 가장 많이 사용되는 손실 함수이다. 기본적으로 모델의 출력 값과 사용자가 원하는 출력 값 사이의 거리 차이를 오차로 사용한다. 그러나 오차를 계산할 때 단순히 거리 차이를 합산하여 평균내게 되면, 거리가 음수로 나왔을 때 합산된 오차가 실제 오차보다 줄어드는 경우가 생긴다. 그렇기 때문에 각 거리 차이를 제곱하여 합산한 후에 평균내는 것이다. 위의 식에서는 1/2를 곱하는 것으로 되어 있으나 실제로는 2가 아닌 전체 데이터의 수를 나누어 1/n을 곱해주어야 한다.



 \- 거리 차이를 제곱하면 좋은 점은, 거리 차이가 작은 데이터와 큰 데이터 오차의 차이가 더욱 커진다는 점이다. 이렇게 되면 어느 부분에서 오차가 두드러지는지 확실히 알 수 있다는 장점이 있다.





- 교차 엔트로피 오차

![img](https://t1.daumcdn.net/cfile/tistory/99C0D73B5A92769625)



 \- 교차 엔트로피 오차(Cross Entropy Error : CEE) : 교차 엔트로피 오차는 기본적으로 분류(Classification) 문제에서 원-핫 인코딩(one-hot encoding)했을 경우에만 사용할 수 있는 오차 계산법이다. 위의 식에서 t값이 원-핫 인코딩된 벡터이고, 거기에다 모델의 출력 값에 자연로그를 취한 것이 곱해지는 형태이다. 결과적으로 교차 엔트로피 오차는 정답일 때의 모델 값에 자연로그를 계산하는 식이 된다.

![img](https://t1.daumcdn.net/cfile/tistory/99384F355A928B1C06)





 \- 분류 문제에서 모델의 출력은 0에서 1 사이이므로 -log(y)의 함수는 위와 같다. 즉, 정답이 1이라고 했을 때, 정답에 근접하는 경우에는 오차가 0에 거의 수렴해간다. 그러나 0에 가까워지면, 정답에 멀어지면 멀어질수록 오차가 기하급수적으로 증가하는 것을 볼 수 있다. 정답에 멀어질수록 큰 패널티를 부여하는 것이다.



 \- 일반적으로 분류 문제에서는 데이터의 출력을 0과 1로 구분하기 위하여 시그모이드(sigmoid) 함수를 사용한다. 시그모이드 함수 식에는 자연상수 e가 포함되므로 기존의 평균 제곱 오차를 사용하면, 매끄럽지 못하고 울퉁불퉁한 표면을 지닌 그래프의 개형이 그려진다. 이런 상태에서 경사 하강법(Gradient Descent Algorithm)을 적용하면 전역 최소점(global minimum)을 찾지 못하고 지역 최소점(local minimum)에 걸릴 가능성이 크다. 이것을 방지하기 위해 자연 상수 e에 반대되는 자연 로그를 모델의 출력 값에 취하는 손실 함수를 사용하는 것이다.



- 손실 함수의 목적

 \- 머신러닝 모델의 최종적인 목적은 높은 정확도를 끌어내는 매개변수(가중치, 편향)를 찾는 것이다. 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. 이 때, 매개변수의 미분(기울기)을 계산하고, 그 미분 값을 토대로 매개변수 값을 갱신하는 과정을 반복한다.



 \- 주요한 점은, 정확도와는 달리 손실 함수는 매개변수의 변화에 따라 연속적으로 변화한다는 점이다. 손실 함수와는 달리 정확도는 매개변수의 변화에 둔감하고, 또한 변화가 있다하여도 불연속적으로 변화하기 때문에 미분을 할 수 없다. 미분이 되지 않으면 최적화를 할 수 없으므로 정확도가 아닌 손실 함수를 지표로 삼아 학습을 해나가는 것이다.



### 경사법

 \- 경사법(Gradient Method) : 머신러닝 모델은 학습 시에 최적의 매개변수(가중치와 편향)를 찾는다. 최적이란, 손실 함수가 최솟값이 될 때의 매개변수 값이다. 따라서 기울기를 이용하여 손실 함수의 최솟값을 찾으려는 것이 바로 경사법이다. 일반적으로 기계 학습을 최적화하는데 사용되고, 특히나 신경망 학습에 많이 사용된다.

![img](https://t1.daumcdn.net/cfile/tistory/9912EE395A9292C918)



 \- 손실함수의 첫 위치에서 기울기를 구한다. 기울기가 (+)이면 음의 방향으로, (-)이면 양의 방향으로 일정 거리만큼 이동한 다음, 그 위치에서의 기울기를 다시 구한다. 그리고 그것을 토대로 다시 일정 거리를 이동하는 것을 반복하여, 손실 함수의 값을 점차적으로 줄여나가는 것이 바로 경사법이다. 경사를 따라 내려가기 때문에, 기본적으로 손실 함수의 그래프 개형이 볼록한 모양일 때에만(convex function) 경사 하강법이 적용 가능하다.



- 학습률

 \- 학습률(Learning Rate) : 위의 식에서 η 기호가 이동하는 거리를 결정한다. 딥러닝에서는 이 것을 학습률이라고 한다. 한 번의 학습으로 매개변수 값을 얼마나 갱신하는가를 정하는 것이 바로 학습률이다. 이것은 시스템이나 모델이 찾는 값이 아니라, 모델을 구성하는 사람이 지정해주어야 하는 값(초매개변수, hyper-parameter)이다. 정답이 있는 것이 아니기 때문에, 환경 등의 요인에 따라 처음에는 0.01 전후로 정하여 학습을 시도해 보는 것이 좋다.



 \- 학습률이 너무 작거나 또 너무 크면 학습이 제대로 이루어지지 않기 때문에, 일반적으로 학습률의 값을 변경해가면서 올바르게 학습하고 있는지를 확인해야한다. 학습률이 너무 크면 손실 함수 값이 줄어들지 않고 오히려 더 커지는 오버슈팅(overshooting) 현상이 발생한다. 반대로 학습률이 너무 작으면 수렴이 늦어지고, 지역 최소점(local minimum)에 걸릴 수 있다.



- 데이터 전처리

 \- 경사 하강법의 안정적인 적용과 학습의 안정성 및 속도를 높이기 위하여, 학습 전에 데이터를 처리해주는 것이 필요하다. 데이터의 중심이 0이 되게 하거나(zero-centered data), 데이터가 항상 일정한 범위 안에 들어오도록 정규화(normalization)하는 것이 이에 해당한다.

출처:[김콜리님의 블로그](<https://kolikim.tistory.com/37?category=733477>)



### 경사하강법에 대한 다른 정리

**경사하강법 (gradient descent)**

말이 어렵지만 쉽게 생각하면 가장 적절한 1차 직선으로 조금씩 변화 (fitting)시켜가는 방식 중 하나이다.

필기식 동영상 강좌인 [이찬우님의 동영상](https://www.youtube.com/watch?v=GmtqOlPYB84)을 먼저 보면 좋을 거 같다. 글로 개념을 이해하는건 삽질이니깐;;

혹시 저 동영상을 보고 이해했다면  아래 나의 설명글은 볼 필요가 없을것이고, 점프하여 바로 파이썬 코드를 살펴보자.



\* 1차만 가능하다는 말은 아니고 이 글에서는 쉽게 1차식으로 설명한다.

![img](https://t1.daumcdn.net/cfile/tistory/2109593B589022690F)



 데이터 포인터들을 나타내는 1차 직선의 방정식을 생각해보자. y = mx + b 가 될 텐데..저 무수한 점들 (빅데이터) 를 일반화 하는 직선의 방정식을 어떤식으로 풀어야할까?  m 과 b 의 값을 어떻게 알맞게 맞춰 (피팅) 갈 수 있을까? 최종 피팅되어  1차 방정식을 알게 된다면 이제 다른 데이터에 대한 예측도 가능 할 것이다.

딥러닝(신경망) 에서 어떻게 초기,결과 데이터를 가지고 중간 파라미터들 (weight) 를 찾아 내는 것과 거의 동일한다.  혹시나 딥러닝을 하실거라면 **경사하강법**은 매우 중요한 기본기 이다.

(예를들어 어떤 방정식을 얻게 된다면 , 해당 사진이 고양이 인지에 대해  예측도 가능 할 것이다.)

![img](https://t1.daumcdn.net/cfile/tistory/2350BD4A589026882B)



일단 아무 직선을 저 데이터 뭉치사이에 그려 넣고, 각 데이터포인터와 직선과의 최단거리를 구한다.그 최단거리의 제곱의 합을 **비용**이라고 부르자. **비용**(오차) 이 클 수록 직선은 저 데이터들을 대표하는 직선이 아닐 것이다.

즉 비용이 최소가 되도록  y = m*x + b 함수에서 m 와 b 의 값을 피팅시켜 나갈 것이다.

여기서 **비용 함수**는 아래와 같다.

![img](https://t1.daumcdn.net/cfile/tistory/2275BA40589028A717)

N :  데이터 샘플들 갯수

Yi :  데이터 샘플중 i 번째의 y 값

MXi + B :  1차직선의방정식의 i 에서 y 값

Error : 비용 (오차)

위 함수에서의 미지수는 m 과 b 이다. 이 미지수를 조절해서 비용(Error) 값이 최소가 되도록 만들어 나간다.



서술해보면 실제데이터와 직선의 차의 제곱의 합의 평균이다. ㅡ.ㅡ;;  흠흠.  역시 수식이 깔끔하다;;;

아무튼 중요한것은 이 비용함수는 2차식이라는 것이고 , 아래와 같이 나타 낼 수 있게 된다.



![img](https://t1.daumcdn.net/cfile/tistory/223B87385890215809)



초기 weight (여기서는 m 또는 b) 에서 경사를 하강시켜가면서 가장 최소의 값을 찾는 것이다. y축이 오차이기 때문에 y가 가장 낮을 수록 가장 적절한 값이기 때문이다.

저 2차선은 비용함수인데 가장낮은 w (여기서는 m 또는 b) 는 무엇인가? 그렇다!!!!! 미분을 해서 0 이 나오는 값!! 즉  최소값이다.(고등학교때 배웠지 않은가? 극대,극소~)

근데 어떻게 찾을까?  해답은 노가다다. 조금씩 변경해가면서 더 작으면 또 조금씩 이동하는것이다. 너무 작게 이동하면 평생 걸릴것이니. 적절히 점프해간다. (이건 과학이 아니라 예술(직감) 이라고들 한다..)

아래 그림에서 왔다리 갔다리 하는 모습을 확인 할 수 있을것이다. 이렇게 한스텝씩 쪼여간다. (물론 좀 더 현명하게 쪼이는 방법도 있긴한데 이 글의 범위를 벗어난다.)



![img](https://t1.daumcdn.net/cfile/tistory/2775173F58902FEF0E)



마지막으로 설명할 부분은 m 과 b 는 각각 따로 편미분을 통해서 최소값을 구하게 된다는 것이다. 당연히 기울기(m) 을 가장 적절하게 바꿔가면서 , 전반적인 높,낮이 (b) 를 바꿔야지 가장 최적의 직선이 될 것 아닌가~

위에 사진에서 W1 은 (m) ,  W2(b) 이며, 초기값 (W1, W2) 에서 변화량(알파) * 방향(도함수값)을 각각 더하고 빼가면서 최적값을 찾는다. 사진에서 알파는 러닝레이트(Learning rate) 로 , 저 값을 조절해서 스텝의 크기가 정해진다.

![img](https://t1.daumcdn.net/cfile/tistory/23086C38589021570E)



단점은 위의 그림처럼 엄한데 가서 최소값을 찾을 수 도 있다는 것이다. 진정한 최소 값은 다른 곳에 있을 수도 있다.

출처: [HAMA님의 블로그](https://hamait.tistory.com/747)



### 손실함수

- 평균제곱오차

```python
import numpy as np

def mean_square_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

사용 결과

```python
>> t = [0,0,1,0,0,0,0,0,0,0]
>> y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
>> mean_square_error(np.array(y),np.array(t))
0.097500000000000031
```



- 교차 엔트로피 오차

```python
import numpy as np

#평균 제곱오차
def mean_square_error(y, t):
    return 0.5 * np.sum((y-t)**2)

#교차 엔트로피 오차
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t*np.log(y+delta))

#출력 결과
>>t = [0,0,1,0,0,0,0,0,0,0]
>>y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]

>>cross_entropy_error(np.array(y),np.array(t))
0.51082545709933802

>> y = [0.1,0.05,0.0,0.0,0.05,0.7,0.0,0.1,0.0,0.0]
>> cross_entropy_error(np.array(y),np.array(t))
16.11809565095832
```



### 경사하강법

- 편미분

```python
def numerical_gradient(f,x):
    h = 1e-4
    grad = np.zeros_like(x) #x와 같은 형상의 배열을 생성

    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x)

        x[idx] = tmp_val-h
        fxh2 = f(x)

        grad[idx] = (fxh1-fxh2) / (2*h)
        x[idx] = tmp_val #값 복원
    return grad

#2차원 벡터를 입력으로 받는 변수 2개의 다항함수
def function_2(x):
    return x[0]**2 + x[1]**2

#출력 결과
>> numerical_gradient(function_2, np.array([3.0,4.0]))
array([ 6.,  8.])
```



- 경사하강법(편미분 벡터 이용)

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f,x)
        x -= lr*grad
    return x

#출력 결과
>> init_x=np.array([3.0,4.0])
>> gradient_descent(function_2,init_x, lr=0.1, step_num=100)
array([  6.19392843e-21,   5.60497583e-21])

#실제 최솟값인 0,0과 비슷한 결과
```

