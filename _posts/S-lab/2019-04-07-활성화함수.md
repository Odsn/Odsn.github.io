---
layout: post
title: 활성화 함수
published: True
category: 실전프로젝트
permalink: /S-lab/:year/:month/:day/:title/
comments: true
---

**■ [용어정리] 활성화 함수(Activation Function)란?**

------

인공지능분야에서 중요한 개념인 활성화 함수란 무엇인지에 대해 정리하고자 합니다.

활성화 함수는 인공지능의 많은 알고리즘에서 다양한 형태로 사용되고 있는데요. 어떠한 활성화 함수를 사용하느냐에 따르 그 출력 값이 달라지기 때문에, 적절한 활성화 함수를 사용하는 것이 매우 중요합니다.

쉽게 말해 활성화 함수란 어떠한 신호를 입력받아 이를 적절한 처리를 하여 출력해주는 함수입니다. 이를 통해 출력된 신호가 다음 단계에서 활성화 되는지를 결정합니다.

그림을 보면 이해가 좀 더 쉬우실 것 같습니다.

(출처:http://cs231n.github.io/neural-networks-1/)

![img](https://t1.daumcdn.net/cfile/tistory/99DAD33359F1FD8A34)



위에서 설명한 활성화 함수의 개념을 표현한 식입니다.

**input data -> {activation function} -> output data**

이렇게 보시면 될 것 같습니다.

이 활성화 함수의 종류는 여러가지가 있는데요. 널리 알려지고, 자주 쓰이는 활성화 함수 몇 가지 종류에 대해서 조금 더 정리해보겠습니다. 해당 함수의 식과 그래프를 중심으로 정리하면 좋을 것 같아 그렇게 진행해볼게요~

**● step function**

먼저 가장 기본이 되는 활성화 함수는 **step function**이라고 할 수 있습니다.

그래프 모양이 계단과 같이 생겼습니다.

이 함수는 임계값을 기준으로 활성화 되거나 혹은 비활성화 되는 형태를 보입니다.

이를 식으로 표현하면 아래와 같습니다.

![img](https://t1.daumcdn.net/cfile/tistory/996C203359F5EE5101)

0을 기준으로 출력이 0이거나 혹은 1로 표현됨이 보이시죠?

쉽게 식만 보셔도 그래프 모양은 계단 형태일 것으로 알 수 있습니다.

![img](https://t1.daumcdn.net/cfile/tistory/99B3CD3359F5F56B36)

Step function의 구현

```python
import numpy as np
import matplotlib.pylab as plt

#계단함수
def step_function(x):
    y = x>0
    return y.astype(np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()
```



**● sigmoid** **function**

다음 활성화 함수는 **sigmoid function**입니다.

이 함수는 항상 0과 1사이의 값만 가질 수 있도록하는 비선형 함수입니다.

step function은 0, 1이라는 출력 값만 가졌지만, 시그모이드는 이 사이에도 연속적인 출력값이 있다는 것으로 봅니다. 미분이 가능하도록 1과 0 사이를 부드럽게 이어주죠.

이를 식으로 표현하면 다음과 같습니다.

![img](https://t1.daumcdn.net/cfile/tistory/9932623359F5F26007)

그래프를 보시면 그 형태에 대해 더욱 이해가 쉬우실 것 같네요.

![img](https://t1.daumcdn.net/cfile/tistory/99F5B53359F5F57D02)

Sigmoid Function의 구현

```python
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1/(1+np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()
```

**● ReLU** **function**

세번째 활성화함수는 **ReLU**라고 불리는 함수입니다.

Rectified Linear Unit의 약자기도 한 이 함수는 선형함수입니다.

sigmoid function의 Gradient Vanishing 문제를 해결하기 위해 최근에 많이 사용되고 있는 활성화 함수 중 하나입니다.

**Gradient Vanishing 문제**란 0과 1사이의 값을 가지는 sigmoid function에서 아주 작은 값을 가질 경우 0에 매우 가까운 값을 가지게 됩니다. ex)0.00001

Gradient Descent를 사용해 Back-propagation 시 각 layer를 지나며 이를 지속적으로 곱해주게 되는데요. 이때 layer가 많을 경우에는 결국 0으로 수렴하는 문제가 발생해요.

그렇기 때문에 layer가 많을 경우 sigmoid가 잘 작동하지 않습니다.

이를 해결하기 위해 생겨난 ReLU의 식은 다음과 같습니다.

![img](https://t1.daumcdn.net/cfile/tistory/9938873359F5F43C09)

이 ReLU의 가장 큰 장점은 위에서 말한 Gradient Vanishing문제를 해결할 수 있으며, 미분이 아주 간단하게 된다는 것입니다. 이 그래프 모양은 다음과 같습니다.

![img](https://t1.daumcdn.net/cfile/tistory/99F8D13359F5F58F32)

Relu function의 구현

```python
import numpy as np
import matplotlib.pylab as plt

def relu(x):
    return np.maximum(0,x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()
```

[활성화 함수 출처](https://zamezzz.tistory.com/215)

[코드 출처](<https://m.blog.naver.com/PostView.nhn?blogId=htk1019&logNo=220965622077&proxyReferer=https%3A%2F%2Fwww.google.com%2F>)

